{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CartPole-v1 Policy Gradient Runner \n",
        "\n",
        "This notebook installs dependencies, runs a quick Actor-Critic training on CartPole-v1, and plots the returns inline. Upload the project files into the working directory or clone the repo before running.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "pip install -q gymnasium[classic-control]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# 1. Clone my repo\n",
        "!git clone https://github.com/NikilTn/CMPE_260_hw3.git\n",
        "\n",
        "%cd CMPE_260_hw3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "#Install dependencies \n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "\n",
        "sys.path.append(os.getcwd())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from actor_critic import ActorCriticAgent\n",
        "from config import get_config\n",
        "from utils import set_seed\n",
        "\n",
        "\n",
        "def train_actor_critic_quick(episodes=300, seed=42):\n",
        "    \"\"\"Train Actor-Critic (TD(0)) and return the trained agent.\"\"\"\n",
        "    config = get_config(\n",
        "        \"actor_critic\",\n",
        "        episodes=episodes,\n",
        "        seed=seed,\n",
        "        log_interval=max(episodes // 5, 1),\n",
        "    )\n",
        "    env = gym.make(\"CartPole-v1\")\n",
        "    set_seed(seed, env)\n",
        "    state_dim = env.observation_space.shape[0]\n",
        "    action_dim = env.action_space.n\n",
        "    agent = ActorCriticAgent(state_dim, action_dim, config)\n",
        "\n",
        "    returns = []\n",
        "    for ep in range(1, episodes + 1):\n",
        "        state, _ = env.reset(seed=seed)\n",
        "        ep_ret = 0.0\n",
        "        for _ in range(config.max_steps_per_episode):\n",
        "            action, log_prob, value = agent.select_action(state)\n",
        "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "            done = terminated or truncated\n",
        "            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(agent.device)\n",
        "            next_state_tensor = torch.FloatTensor(next_state).unsqueeze(0).to(agent.device)\n",
        "            agent.update_step(state_tensor, log_prob, reward, next_state_tensor, done, value)\n",
        "            ep_ret += reward\n",
        "            state = next_state\n",
        "            if done:\n",
        "                break\n",
        "        returns.append(ep_ret)\n",
        "        if ep % max(episodes // 5, 1) == 0:\n",
        "            print(f\"Episode {ep}: return={ep_ret:.1f}, avg(50)={np.mean(returns[-50:]):.1f}\")\n",
        "    env.close()\n",
        "    return agent, returns\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train a quick Actor-Critic policy\n",
        "agent, returns = train_actor_critic_quick(episodes=300, seed=42)\n",
        "print(f\"Final avg return (last 50): {np.mean(returns[-50:]):.1f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(8,4))\n",
        "plt.plot(returns, label='Episode return')\n",
        "if len(returns) > 10:\n",
        "    window = min(50, len(returns))\n",
        "    ma = np.convolve(returns, np.ones(window)/window, mode='valid')\n",
        "    plt.plot(range(window-1, len(returns)), ma, label=f'MA({window})')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Return')\n",
        "plt.title('Actor-Critic on CartPole-v1')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
